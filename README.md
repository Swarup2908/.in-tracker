# (.in) Domain Registration Tracker ğŸ“ˆ

This project automatically scrapes, parses, and records the monthly count of newly registered **`.in` domains** from [registry.in](https://www.registry.in/domain-archives).  
It downloads official monthly PDF reports, extracts the count, and appends it to a CSV file. The task is automated using **Celery + Redis**, scheduled to run **on the 3rd of every month**.

---

## ğŸ“¦ Features

- ğŸ”— Scrapes PDF reports from registry.in
- ğŸ“„ Parses `.pdf` files using `pdfplumber`
- ğŸ“Š Appends clean data to `domain_data.csv`
- ğŸ§  Avoids duplicate entries intelligently
- â° Automated monthly execution using Celery

---

## ğŸ—‚ Project Structure

```
.
â”œâ”€â”€ scraper.py          # PDF link scraper
â”œâ”€â”€ parser.py           # PDF content parser
â”œâ”€â”€ main.py             # Manual run trigger (optional)
â”œâ”€â”€ tasks.py            # Celery task logic
â”œâ”€â”€ celeryconfig.py     # Celery Beat scheduler config
â”œâ”€â”€ domain_data.csv     # Output data file
â”œâ”€â”€ reports/            # Folder with downloaded PDFs
â”œâ”€â”€ requirements.txt    # Dependencies
â””â”€â”€ README.md           # Project documentation
```

---

## ğŸ“¥ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/in-domain-tracker.git
cd in-domain-tracker
```

### 2. Create Virtual Environment (Optional but Recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Run

### ğŸ”¹ Manually (Optional)

You can test the scraper and parser manually:

```bash
python main.py
```

### ğŸ”¹ Automatically with Celery

1. **Make sure Redis server is running**
   (Install via `sudo apt install redis` or follow official docs)

2. **Start the Celery Worker:**
   ```bash
   celery -A tasks worker --loglevel=info
   ```

3. **Start the Celery Beat Scheduler:**
   ```bash
   celery -A tasks beat --loglevel=info
   ```

---

## ğŸ§ª Dependencies

Your `requirements.txt` contains:

```txt
requests
beautifulsoup4
pdfplumber
celery
redis
```

These libraries are responsible for:
- **requests** â€“ Making HTTP requests to fetch pages
- **beautifulsoup4** â€“ Parsing HTML to extract PDF links
- **pdfplumber** â€“ Reading and extracting text from PDF reports
- **celery** â€“ Running asynchronous tasks
- **redis** â€“ Celery's backend broker for task scheduling

---

## ğŸ“Š Output

All parsed data is saved in `domain_data.csv` as:

```csv
Month,New Domains
June 2024,23112
May 2024,21753
...
```

---

## ğŸ“… Automation Schedule

This project is configured to automatically update on the **3rd of every month**.
You can modify the schedule in `celeryconfig.py` if needed.

---
